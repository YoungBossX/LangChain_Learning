{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af748e8f",
   "metadata": {},
   "source": [
    "# 1、字符串输出解析器 StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8368c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "大语言模型是指一种能够生成自然语言文本的机器学习模型，这些模型通常基于深层神经网络结构，如变换器（Transformer）架构，具有大量的参数（通常超过百万级甚至亿级参数）。大语言模型通过对大量文本数据进行训练，可以理解和生成与人类相似的自然语言文本，具备强大的语言理解和生成能力。\n",
      "\n",
      "大语言模型能够完成的任务范围广泛，包括但不限于文本补全、机器翻译、情感分析、问答系统、文本分类、摘要生成等。在近年来自然语言处理领域取得了重要进展，成为诸多文本生成任务的有力工具。代表性的大语言模型包括但不限于：阿里云的通义千问、谷歌的LaMDA、百度的文心一言、阿里云的通义万相、百度ERNIE等。\n"
     ]
    }
   ],
   "source": [
    "# 1、获取大模型\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, XMLOutputParser\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.utils import pre_init\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"LLM_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"LLM_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model=os.getenv(\"LLM_MODEL_ID\"))\n",
    "\n",
    "# 2、调用大模型\n",
    "response = chat_model.invoke(\"什么是大语言模型？\")\n",
    "# print(type(response))   # AIMessage\n",
    "\n",
    "# 3、如何获取一个字符串的输出结果呢？\n",
    "# # 方式1：自己调用输出结果的content\n",
    "# print(response.content)\n",
    "\n",
    "# 方式2：使用StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "str_response = parser.invoke(response)\n",
    "print(type(str_response))  # <class 'str'>\n",
    "print(str_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2e113",
   "metadata": {},
   "source": [
    "# 2、JsonOutputParser : Json输出解析器\n",
    "\n",
    "方式1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3986bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': '人工智能用英文怎么说？', 'a': '人工智能用英文说为Artificial Intelligence (AI)'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_model = ChatOpenAI(model=os.getenv(\"LLM_MODEL_ID\"))\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个靠谱的{role}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 正确的：\n",
    "prompt = chat_prompt_template.invoke(\n",
    "    input={\"role\": \"人工智能专家\", \"question\": \"人工智能用英文怎么说？问题用q表示，答案用a表示，返回一个JSON格式的数据\"})\n",
    "\n",
    "# 错误的：\n",
    "# prompt = chat_prompt_template.invoke(input={\"role\":\"人工智能专家\",\"question\":\"人工智能用英文怎么说？\"})\n",
    "\n",
    "response = chat_model.invoke(prompt)\n",
    "# print(response.content)\n",
    "\n",
    "# 获取一个JsonOutputParser的实例\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "json_result = parser.invoke(response)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf04f0",
   "metadata": {},
   "source": [
    "方式2：\n",
    "\n",
    "举例1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b48e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a JSON object.\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser()\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce78c6",
   "metadata": {},
   "source": [
    "举例2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e24802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"joke\": \"为什么电脑经常生病？因为窗户（Windows）总是开着！\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 52, 'total_tokens': 78, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen2.5-7B-Instruct', 'system_fingerprint': '', 'id': '019bf9036ae9f033638cd9c64685e96b', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bf903-6d88-71c3-8ff1-7690fd2a1514-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 52, 'output_tokens': 26, 'total_tokens': 78, 'input_token_details': {}, 'output_token_details': {}}\n",
      "{'joke': '为什么电脑经常生病？因为窗户（Windows）总是开着！'}\n"
     ]
    }
   ],
   "source": [
    "# 引入依赖包\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化语言模型\n",
    "chat_model = ChatOpenAI(model=os.getenv(\"LLM_MODEL_ID\"))\n",
    "\n",
    "joke_query = \"告诉我一个笑话。\"\n",
    "\n",
    "# 定义Json解析器\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 以PromptTemplate为例\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"回答用户的查询\\n 满足的格式为{format_instructions}\\n 问题为{question}\\n\",\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke(input={\"question\": joke_query})\n",
    "response = chat_model.invoke(prompt)\n",
    "print(response)\n",
    "\n",
    "json_result = parser.invoke(response)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cb5aa",
   "metadata": {},
   "source": [
    "知识的拓展： |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665f97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': '人工智能用英文怎么说？', 'a': 'Artificial Intelligence (AI)'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_model = ChatOpenAI(model=os.getenv(\"LLM_MODEL_ID\"))\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个靠谱的{role}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 获取一个JsonOutputParser的实例\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 写法1：\n",
    "# prompt = chat_prompt_template.invoke(input={\"role\":\"人工智能专家\",\"question\":\"人工智能用英文怎么说？问题用q表示，答案用a表示，返回一个JSON格式的数据\"})\n",
    "#\n",
    "# response = chat_model.invoke(prompt)\n",
    "#\n",
    "# json_result = parser.invoke(response)\n",
    "# print(json_result)\n",
    "\n",
    "# 写法2：\n",
    "chain = chat_prompt_template | chat_model | parser\n",
    "json_result1 = chain.invoke(\n",
    "    input={\"role\": \"人工智能专家\", \"question\": \"人工智能用英文怎么说？问题用q表示，答案用a表示，返回一个JSON格式的数据\"}\n",
    "    )\n",
    "print(json_result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743d2e3",
   "metadata": {},
   "source": [
    "针对于举例2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4a6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': '为什么电脑经常生病？因为窗户（windings）总是开着！'}\n"
     ]
    }
   ],
   "source": [
    "# 引入依赖包\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化语言模型\n",
    "chat_model = ChatOpenAI(model=os.getenv(\"LLM_MODEL_ID\"))\n",
    "\n",
    "joke_query = \"告诉我一个笑话。\"\n",
    "\n",
    "# 定义Json解析器\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 以PromptTemplate为例\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"回答用户的查询\\n 满足的格式为{format_instructions}\\n 问题为{question}\\n\",\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "# 写法1：\n",
    "# prompt = prompt_template.invoke(input={\"question\":joke_query})\n",
    "# response = chat_model.invoke(prompt)\n",
    "# json_result = parser.invoke(response)\n",
    "\n",
    "chain = prompt_template | chat_model | parser\n",
    "json_result = chain.invoke(input={\"question\": joke_query})\n",
    "print(json_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
